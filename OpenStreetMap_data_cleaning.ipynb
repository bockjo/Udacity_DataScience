{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenStreetMap Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Johannes Bock, September 18, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following project I will choose an area of the world in https://www.openstreetmap.org and use data munging techniques, such as assessing the quality of the data for validity, accuracy, completeness, consistency and uniformity, to clean the OpenStreetMap data. Moreover, after thoroughly auditing and cleaning the dataset, I will build my own local SQL database, import the OSM data and explore the database by running some SQL queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Area\n",
    "\n",
    "In the following project I chose to look at OSM data for my hometown ([Karlsruhe, Germany](http://www.openstreetmap.org/export#map=13/49.0118/8.4131)) and investigated the quality of the available information on Karlsruhe in OSM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems Encountered in the Map\n",
    "\n",
    "First, I downloaded and audied the OSM data for Karlsruhe. In general, the OSM data for my hometown already is quite clean, since I could not find any problems with inconsistent streetnames or postcodes, for instance. However, I noticed three problems with the data, which I will discuss in the following order:\n",
    "\n",
    "1) OSM Keys with unexpected format <br>\n",
    "2) Inconsistent formatting of phone numbers <br>\n",
    "3) Inconsistent names for a local company <br>\n",
    "4) Inconsistent date formats in OSM key = \"start_date\"\n",
    "\n",
    "#### Keys with unexpected format\n",
    "Looking at some keys in the dataset, I have found 16342 entries with an unexpected format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Result\n",
    "{'lower': 344630, 'lower_colon': 242823, 'other': 16342, 'problemchars': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at some examples which were labeled \"other\", I could see some patterns here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<tag k=\"railway:signal:crossing_distant\" v=\"DE-ESO:bÃ¼2\" />\n",
    "<tag k=\"parking:condition:left\" v=\"free\" />\n",
    "<tag k=\"traffic_sign:backward\" v=\"none\" />\n",
    "<tag k=\"ref:hafas:DB\" v=\"723635\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Researching these keys reveals that they seem to be very specialized keys for describing ways in OSM and sometimes they are not even part of the internationally standardized OSM key definitions (e.g. [country-specific signal tagging](http://wiki.openstreetmap.org/wiki/Tag:railway%3Dsignal)). Since these keys are not relevant for the subsequent analysis and do not contain meaningful information for my purposes, I will just ignore these highly specialized keys and will not add them to the database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inconsistent formatting of phone numbers\n",
    "In the OSM dataset there are some datafields containing phone numbers. These include the OSM keys \"phone\" and \"contact:phone\". Therefore, I decided to audit these fields and check whether there are inconsistencies. The following code describes my analysis of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Expected_format': 648,\n",
      " 'Whitespace_problem': 202,\n",
      " 'no_country_code': 23,\n",
      " 'numbers_only': 285,\n",
      " 'other': 161}\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "            \n",
    "def audit_phone(element,count):\n",
    "    phone_pattern = re.compile(r\"^(\\+49)\\s[0-9]*\\s[0-9]*$\",re.I)\n",
    "    whitespace_end = re.compile(r\"^(\\+49)\\s[0-9]*\\s[0-9\\s|\\-]*$\")\n",
    "    whitespace_start = re.compile(r\"^(\\+49)[\\-|\\s*][0-9]*[\\-|\\s*][0-9]*$\")\n",
    "    no_country_code = re.compile(r\"^([0-9])*\\s[0-9]*$\",re.I)\n",
    "    numbers_only = re.compile(r\"^[0-9]*$\",re.I)\n",
    "    if \"phone\" in element.attrib[\"k\"]:\n",
    "        if phone_pattern.search(element.attrib[\"v\"]):\n",
    "            count[\"Expected_format\"]+=1\n",
    "        elif whitespace_end.search(element.attrib[\"v\"]) and not phone_pattern.search(element.attrib[\"v\"]):\n",
    "            count[\"Whitespace_problem\"]+=1\n",
    "        elif whitespace_start.search(element.attrib[\"v\"]) and not phone_pattern.search(element.attrib[\"v\"]):         \n",
    "            count[\"Whitespace_problem\"]+=1\n",
    "        elif no_country_code.search(element.attrib[\"v\"]):\n",
    "            count[\"no_country_code\"] += 1\n",
    "        elif numbers_only.search(element.attrib[\"v\"]):\n",
    "            count[\"numbers_only\"]+=1\n",
    "            #return element.attrib[\"v\"]\n",
    "        else:\n",
    "            count[\"other\"] += 1             \n",
    "            return element.attrib[\"v\"]\n",
    "        \n",
    "def process_phone(filename,tags=(\"tag\")):\n",
    "    unexpected_phone_numbers = set()\n",
    "    count = {\"Expected_format\": 0 , \"Whitespace_problem\":0, \"no_country_code\":0, \"numbers_only\":0,\"other\":0}\n",
    "    for element in get_element(filename, tags):\n",
    "        number = audit_phone(element,count)\n",
    "        unexpected_phone_numbers.add(number)\n",
    "    return unexpected_phone_numbers, count\n",
    "\n",
    "data = \"Karlsruhe.xml\"\n",
    "bad_numbers, count = process_phone(data)\n",
    "\n",
    "pprint.pprint(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above shows the count of phone numbers that either met or did not meet the following expected format:<br><br>\n",
    "\"(Country_code: +49) (local_code: any combination of numbers) (phone_number: any combination of numbers)\"<br><br>\n",
    "From the results summary one can see that the majority of 648 data entries meet this specific format.Some 202 phone numbers only have a whitespace problem which can be cleaned up programmatically (See below). 23 numbers do not contain country codes and unfortunately, I can do little about that but going through them manually and gues the country codes. 285 datapoints are only a sequence of numbers. Cleaning these datapoints requires manual work as well, since it is hard to separate all of them (into country_code) : (local-code) : (phone-number) programmatically assuming there are various different country and local codes potentially present in the data. However, I will try to extract all phone numbers with local-code from Karlsruhe and bring them in the expected format. Finally there are some 161 datapoints which I have labeled \"other\" meaning that the format of these phone numbers follows various different patterns. Below I have printed out some exmaples of these \"other\" formats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'+49 /(0) 721 925 - 5700'\n",
      "'+49 (0)721 86 48 97 84'\n",
      "'+49 (0) 721 23 280'\n",
      "'yes'\n",
      "'+49721484040'\n"
     ]
    }
   ],
   "source": [
    "z=0\n",
    "for e in bad_numbers:\n",
    "    if z == 5:\n",
    "        break\n",
    "    else:\n",
    "        pprint.pprint(e)\n",
    "        z+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output above, one can see that the format of these numbers varies a lot. Hence, in order to clean these datapoints I could either find some more patterns in the formatting or go through them manually.<br> The output also shows the value \"yes\" which is not a phone-number at all. Investigating this datapoint, I found that a wrong OSM key was included in the analysis which should be excluded in following investigations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inconsistent name for a local company\n",
    "Another problem which I encountered in the dataset is an inconsistent use of different names for the same company. The company \"Aldi\" is a large German supermarket chain with various stores all over Karlsruhe. With my analysis I could find some problems with the inconsistent use of the company's name in the OSM dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(set([None,\n",
      "      u'brand=Aldi S\\xfcd',\n",
      "      'name=ALDI',\n",
      "      'name=Aldi',\n",
      "      u'name=Aldi S\\xfcd',\n",
      "      'operator=ALDI GmbH & Co. KG, Rastatt',\n",
      "      'operator=Aldi',\n",
      "      u'operator=Aldi S\\xfcd']),\n",
      " {'Anzahl': 22})\n"
     ]
    }
   ],
   "source": [
    "def audit_company_name(element, count):\n",
    "    company_pattern = re.compile(r\"^(Aldi)(\\s|$)\",re.I)\n",
    "    if element.attrib[\"k\"] in [\"name\", \"brand\",\"operator\"]:\n",
    "        if company_pattern.search(element.attrib[\"v\"]):\n",
    "            name = element.attrib[\"k\"] + \"=\" + element.attrib[\"v\"]\n",
    "            count[\"Anzahl\"] += 1\n",
    "            return name\n",
    "    \n",
    "def process_company_audit(filename,tags=(\"tag\")):\n",
    "    names = set()\n",
    "    count = {\"Anzahl\":0}\n",
    "    for element in get_element(filename, tags):\n",
    "        name = audit_company_name(element,count)\n",
    "        names.add(name)\n",
    "    return names, count\n",
    "\n",
    "pprint.pprint(process_company_audit(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above shows all different kinds of ways how the company's name is being spelled in the OSM dataset. Below I will remove this inconsistency and name the company consistently throughout the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inconsistent date formats in key = \"start_date\"\n",
    "\n",
    "Looking at the date formats used for the OSM key \"start_date\" I found that quite a lot of the data entries do not follow the expected format \"YYYY-MM-DD\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Expected': 480, 'other': 1158}\n"
     ]
    }
   ],
   "source": [
    "def audit_date(element,count):\n",
    "    date_pattern = re.compile(r\"^([0-9]{4,4}\\-[0-9]{2,2}\\-[0-9]{2,2}$)\")\n",
    "    if element.attrib[\"k\"] == \"start_date\":\n",
    "        if date_pattern.search(element.attrib[\"v\"]):\n",
    "            count[\"Expected\"]+=1\n",
    "        else:\n",
    "            count[\"other\"]+=1\n",
    "            return element.attrib[\"v\"]\n",
    "            \n",
    "def process_date(filename,tags=(\"tag\")):\n",
    "    dates = set()\n",
    "    count = {\"Expected\":0,\"other\":0}\n",
    "    for element in get_element(filename, tags):\n",
    "        date = audit_date(element,count)\n",
    "        dates.add(date)\n",
    "    return dates, count\n",
    "\n",
    "dates, count = process_date(data)\n",
    "pprint.pprint(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above shows that a majority of 1.158 datapoints do not have the expected format. However, looking at some examples with an unexpected date format I could quickly see a pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<tag k=\"start_date\" v=\"1908\" />\n",
    "<tag k=\"start_date\" v=\"2012-06\" />\n",
    "<tag k=\"start_date\" v=\"~2015\" />\n",
    "<tag k=\"start_date\" v=\"09/2014\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the examples above, it is clear that the problem is not necessarily caused by an inconsistent use of different date formats (even though this is potentially a problem in a few cases) but rather a problem of missing data or missing information about the exact \"start_date\" including day and month information. Hence, cleaning the date by forcing all datapoints into a standardized format does not improve the data quality in the sense that we get more accurate information. Therefore, I assume that the value-add of cleaning the \"start_date\" data is negligible in this case and thus, I will not clean this datafield."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Migrating the OSM dataset to CSV\n",
    "\n",
    "After having identified some problems with the OSM dataset, I have first migrated the OSM dataset to a CSV files before starting to clean the data. The csv dataset does not include any unusual OSM keys as mentioned earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "Having already identified causes for some data quality problems in the OSM dataset, I can now continue with defining operations to clean the dataset and get rid of the encountered data quality issues.\n",
    "\n",
    "#### Programmatically cleaning phone numbers\n",
    "\n",
    "The following operation updates all phone numbers with whitespace problems, such that: â+49 721 133-5630â becomes: â+49 721 1335630â, for instance. Also phone numbers that contain only digits and the local code \"0721\" at the beginning have been cleaned. For the \"other\" cases mentioned above, automatic cleaning is not feasible, hence these types of phones numbers are not changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_number(number,count):\n",
    "    expected_pattern = re.compile(r\"^(\\+49)\\s[0-9]*\\s[0-9]*$\",re.I)\n",
    "    whitespace_end = re.compile(r\"^(\\+49)\\s[0-9]*\\s[0-9\\s|\\-]*$\")\n",
    "    whitespace_start = re.compile(r\"^(\\+49)[\\-|\\s*][0-9]*[\\-|\\s*][0-9]*$\")\n",
    "    numbers_only = re.compile(r\"^[0-9]*$\",re.I)\n",
    "    \n",
    "    number = re.sub(' +',' ',number)\n",
    "    \n",
    "    if whitespace_end.search(number) and not expected_pattern.search(number):\n",
    "        pattern = re.search(r\"^(.+?)\\s(.+?)\\s(.+?)$\",number)\n",
    "        country_code = pattern.group(1)\n",
    "        local_code = pattern.group(2)\n",
    "        dial = \"\".join((\"\".join(pattern.group(3).split())).split(\"-\"))\n",
    "        new = country_code + \" \" + local_code + \" \" + dial\n",
    "        count[\"cleaned\"] += 1         \n",
    "        return new\n",
    "        \n",
    "    elif whitespace_start.search(number) and not expected_pattern.search(number):\n",
    "        count[\"cleaned\"] += 1          \n",
    "        return re.sub('-',' ',number) \n",
    "        \n",
    "    elif numbers_only.search(number):\n",
    "        start_local_code = re.compile(r\"^(0721)\")\n",
    "        \n",
    "        if start_local_code.search(number):\n",
    "            no_pattern = re.search(r\"^(0721)(.+?)$\",number)\n",
    "            dial = no_pattern.group(2)\n",
    "            number = \"+49 \" + \"721 \" + dial\n",
    "            count[\"cleaned\"] += 1             \n",
    "            return number\n",
    "    else:\n",
    "        return number\n",
    "       \n",
    "# update_number: Takes as input a list of csv filenames. Reads in the csv files, and cleans\n",
    "        # for all keys that contain \"phone\" the value using update_number and saves the cleaned csv.\n",
    "        \n",
    "def clean_phone_numbers(files):    \n",
    "    count = {\"cleaned\":0}   \n",
    "    for datei in files:\n",
    "        filename = datei\n",
    "        tempfile = NamedTemporaryFile(delete=False)\n",
    "        \n",
    "        with open(filename,\"rb\") as csvfile,tempfile:\n",
    "            reader = csv.DictReader(csvfile,delimiter=\",\")\n",
    "            writer = csv.DictWriter(tempfile,delimiter=\",\",fieldnames = reader.fieldnames)\n",
    "            writer.writeheader()\n",
    "            for row in reader:\n",
    "                if \"phone\" in row['key']:\n",
    "                    row['value'] = update_number(row['value'],count)\n",
    "                writer.writerow(row)\n",
    "        shutil.move(tempfile.name, filename)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this cleaning operation a total of 220 phone numbers have been cleaned and changed into a consistent formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{'cleaned': 220}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Programmatically cleaning inconsistent company naming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have a consistent naming for all Aldi branches in Karlsruhe, I have defined the following operation which forces all firm names to be the same, such that: \"ALDI GmbH & Co. KG, Rastatt\" becomes: \"Aldi\", for instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_firm_name(name,count):\n",
    "    company_pattern = re.compile(r\"^(Aldi)(\\s|$)\",re.I)    \n",
    "    if company_pattern.search(name):\n",
    "        name = \"Aldi\"        \n",
    "        count[\"cleaned\"] += 1 \n",
    "        return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A total of 22 datafields were cleand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{'cleaned': 22}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Migrating the OSM dataset from CSV to a SQL database\n",
    "\n",
    "After having cleaned the dataset and having updated the csv files, I migrated the OSM dataset to a SQL database. For adding the OSM dataset for Karlsruhe to a SQL database I have used [the schema provided by Stephen Welch](https://gist.github.com/swwelch/f1144229848b407e0a5d13fcb7fbbd6f).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Overview & Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea about my hometown and some facts and figures, I will explore the OSM dataset in the following section using SQL queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Karlsruhe.osm ......... 161 MB\n",
    "Karlsruhe.db .......... 89 MB\n",
    "nodes.csv ............. 49 MB\n",
    "nodes_tags.csv ........ 5 MB\n",
    "ways.csv .............. 6 MB\n",
    "ways_tags.csv ......... 14 MB\n",
    "ways_nodes.csv ......... 21 MB  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of nodes in Karlsruhe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SQL Query\n",
    "SELECT COUNT(*) FROM nodes;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "630960"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of ways in Karlsruhe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SQL Query\n",
    "SELECT COUNT(*) FROM ways;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "111083"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 contributing users to OSM Karlsruhe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SQL Query\n",
    "SELECT e.user, COUNT(*) as num\n",
    "   FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e\n",
    "   GROUP BY e.user\n",
    "   ORDER BY num DESC\n",
    "   LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ARWIE\t\t201574\n",
    "mapper999\t171732\n",
    "Blunauer\t112203\n",
    "leloop\t\t32762\n",
    "jlcod\t\t19166\n",
    "maction\t\t14433\n",
    "aluka\t\t13564\n",
    "drhzbg\t\t13553\n",
    "B-Rabbit\t9074\n",
    "Nakaner\t\t8961"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most popular cuisine in Karlsruhe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SQL Query\n",
    "SELECT nodes_tags.value, COUNT(*) as num\n",
    "FROM nodes_tags \n",
    "    JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value='restaurant') i\n",
    "    ON nodes_tags.id=i.id\n",
    "WHERE nodes_tags.key='cuisine'\n",
    "GROUP BY nodes_tags.value\n",
    "ORDER BY num DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "italian\t\t39\n",
    "german\t\t16\n",
    "greek\t\t16\n",
    "regional\t15\n",
    "pizza\t\t14\n",
    "indian\t\t11\n",
    "thai\t\t10\n",
    "chinese\t\t8\n",
    "burger\t\t5\n",
    "turkish\t\t5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most common leisure facilities Karlsruhe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SQL Query\n",
    "SELECT nodes_tags.value, COUNT(*) as num FROM nodes_tags\n",
    "WHERE nodes_tags.key='leisure'\n",
    "GROUP BY nodes_tags.value\n",
    "ORDER BY num DESC\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "playground\t\t    134\n",
    "pitch\t\t\t    32\n",
    "picnic_table\t\t25\n",
    "sports_centre\t\t24\n",
    "outdoor_seating\t\t17\n",
    "fitness_station\t\t13\n",
    "adult_gaming_centre\t6\n",
    "dance\t\t\t    5\n",
    "fitness_centre\t\t5\n",
    "sauna\t\t\t    4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of nightclubs in Karlsruhe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SQL Query\n",
    "SELECT COUNT(nodes_tags.id) FROM nodes_tags\n",
    "\tJOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value='nightclub') i\n",
    "    \tON nodes_tags.id=i.id\n",
    "WHERE nodes_tags.key='amenity';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most popular tourism sites in Karlsruhe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SQL Query\n",
    "SELECT nodes_tags.value, COUNT(*) as num FROM nodes_tags\n",
    "   WHERE nodes_tags.key='tourism'\n",
    "   GROUP BY nodes_tags.value\n",
    "   ORDER BY num DESC\n",
    "   LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "information\t\t\t217\n",
    "artwork\t\t\t\t102\n",
    "hotel\t\t\t\t35\n",
    "picnic_site\t\t\t14\n",
    "attraction\t\t\t10\n",
    "museum\t\t\t\t9\n",
    "viewpoint\t\t\t6\n",
    "guest_house\t\t\t3\n",
    "bed_and_breakfast\t1\n",
    "hostel\t\t\t\t1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of historic places in Karlsruhe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SQL Query\n",
    "SELECT COUNT(nodes_tags.id) FROM nodes_tags\n",
    "\tJOIN (SELECT DISTINCT(id) FROM nodes_tags) i\n",
    "    \tON nodes_tags.id=i.id\n",
    "WHERE nodes_tags.key='historic';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "368"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion & additional ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrangling the OSM dataset for my hometown was an interesting experience, since the open-source concept of OSM makes an exiting data source available to everybody. At the end of the Data Wrangling process of auditing, cleaning and migrating the data, I conducted some data exploration and found some interesting facts & figures about Karlsruhe.<br>\n",
    "However, data quality concerns for this open source content remain. This is because it is fairly hard to assess the completeness of the dataset without a benchmark. It is however, very likely that OSM data is incomplete. Hence, this is a problem which should always be kept in mind when dealing with OSM data. In order to improve the completeness assessment of the OSM data, one could compare its information to datasets of other providers such as Google Maps. However, Google data cannot be accessed as easy as OSM data since it is not open-source."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
